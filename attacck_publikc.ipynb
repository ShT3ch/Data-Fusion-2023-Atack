{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67a9767-ff36-4247-9f08-3aee035f7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import TransactionsRnn, TransactionsDataset, process_for_nn, get_dataloader\n",
    "import torch\n",
    "import pickle\n",
    "from itertools import islice\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from check_budget import check_budget # функция проверки бюджета. Проверяйте допустимость решения до сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e376fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_path = \"nn_bins.pickle\" # путь до файла с бинами после тренировки модели (nn_bins.pickle)\n",
    "model_path = \"nn_weights.ckpt\" # путь до файла с весами нейронной сети (nn_weights.ckpt)\n",
    "quantiles_path = \"quantiles.json\" # путь до файла с квантилями для таргета (quantiles.pickle)\n",
    "BUDGET = 10 # разрешенное количество изменений транзакций для каждого пользователя\n",
    "\n",
    "output_path = \"non_naive_submission.csv\" # куда сохранить атакованные транзакции\n",
    "transactions_path = \"sample_submission.csv\"    # путь до файла с транзакциями, которые атакуются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6833df9-3405-4a84-b771-e7bd444fb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8771a0d2-e11c-4dd2-9722-2f4a33e43848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 20230206\n"
     ]
    }
   ],
   "source": [
    "rnn = TransactionsRnn()\n",
    "rnn.load_state_dict(torch.load(model_path))\n",
    "pl.seed_everything(20230206)\n",
    "rnn = rnn.eval()\n",
    "rnn._gru.train()\n",
    "\n",
    "rnn = rnn.to(device)\n",
    "\n",
    "with open(bins_path, \"rb\") as f:\n",
    "    bins = pickle.load(f)\n",
    "\n",
    "features = bins.pop(\"features\")\n",
    "features_to_operate = ['mcc_code', 'transaction_amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42411273-8fa8-4121-86a5-65ce9808913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(quantiles_path, \"rb\") as f:\n",
    "    quantiles = json.load(f)\n",
    "\n",
    "    \n",
    "# преобразовываем бюджетные ограничения так, чтобы потом было удобно\n",
    "negative_q_df = pd.DataFrame([\n",
    "    list(quantiles['negative']['min'].keys())], index=['mcc_code']).T\n",
    "negative_q_df['minqua'] = [quantiles['negative']['min'][k] for k in negative_q_df.mcc_code]\n",
    "negative_q_df['maxqua'] = [quantiles['negative']['max'][k] for k in negative_q_df.mcc_code]\n",
    "\n",
    "positive_q_df = pd.DataFrame([\n",
    "    list(quantiles['positive']['min'].keys())], index=['mcc_code']).T\n",
    "positive_q_df['minqua'] = [quantiles['positive']['min'][k] for k in positive_q_df.mcc_code]\n",
    "positive_q_df['maxqua'] = [quantiles['positive']['max'][k] for k in positive_q_df.mcc_code]\n",
    "\n",
    "temp_bins_mapper = {\n",
    "    'minqua': 'transaction_amt',\n",
    "    'maxqua': 'transaction_amt'\n",
    "}\n",
    "\n",
    "def digitize_cat(df):\n",
    "    for dense_col in list(bins.keys()) + ['minqua', 'maxqua']:\n",
    "        if dense_col in df:\n",
    "            if dense_col in [\"transaction_amt\", 'minqua', 'maxqua']:\n",
    "                df[dense_col+'_bin'] = pd.cut(df[dense_col], bins=bins[temp_bins_mapper.get(dense_col, dense_col)], labels=False).astype(int)\n",
    "            else:\n",
    "                df[dense_col+'_bin'] = pd.cut(\n",
    "                    df[dense_col].astype(float).astype(int), bins=bins[temp_bins_mapper.get(dense_col, dense_col)], labels=False,\n",
    "                ).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "negative_q_df_after_digitize = negative_q_df.pipe(digitize_cat)\n",
    "negative_q_df_after_digitize = negative_q_df_after_digitize.set_index('mcc_code')\n",
    "\n",
    "positive_q_df_after_digitize = positive_q_df.pipe(digitize_cat)\n",
    "positive_q_df_after_digitize = positive_q_df_after_digitize.set_index('mcc_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d925f6d5-276d-415f-aed2-d88486b0be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# это нам понадобится тоже потом, для восстановления айдишников эмбеддингов в номера транзакций\n",
    "original2bin = {\n",
    "    'mcc_code': {\n",
    "        mcc:(negative_q_df_after_digitize.mcc_code_bin.get(mcc) or positive_q_df_after_digitize.mcc_code_bin.get(mcc)) \n",
    "        for mcc in set(list(negative_q_df_after_digitize.index) + list(positive_q_df_after_digitize.index))\n",
    "    }\n",
    "}\n",
    "\n",
    "def found_source(original2bin, feature, bin_position):\n",
    "    for bin_key in original2bin[feature].keys():\n",
    "        if original2bin[feature][bin_key] == bin_position:\n",
    "            return bin_key\n",
    "    return -1  # dirty hack\n",
    "    raise ValueError(f'impossible fo find in {feature} list bin {bin_position}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089865a5-bee9-4f36-90bb-fd0b5bce3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод находит лучшую замену из имеющихся эмбеддингов по направлению, указанному якобианом\n",
    "def _compute_best_option_for_change_emb(original_emb, desired, emb, feature_name):\n",
    "    desired = desired.reshape(desired.shape[0], desired.shape[1], 1, desired.shape[2])\n",
    "    original_emb = original_emb.reshape(original_emb.shape[0], original_emb.shape[1], 1, original_emb.shape[2])\n",
    "    embs2choose = emb.weight.repeat(original_emb.shape[0], original_emb.shape[1], 1, 1)\n",
    "\n",
    "    similarity2possible_from_desired = -100500 * torch.ones(\n",
    "        (\n",
    "            original_emb.shape[0], \n",
    "            original_emb.shape[1], \n",
    "            emb.weight.shape[0]\n",
    "        ),\n",
    "        device=emb.weight.device\n",
    "    )\n",
    "    \n",
    "    desired_direction = desired / torch.norm(desired, dim=-1, keepdim=True)\n",
    "    possible_steps_direction = embs2choose - original_emb\n",
    "    possible_steps_direction = possible_steps_direction / (torch.norm(possible_steps_direction, dim=-1, keepdim=True) + 0.1)\n",
    "    \n",
    "    for batch_item in range(original_emb.shape[0]):\n",
    "        similarity2possible_from_desired[batch_item] = torch.bmm(\n",
    "            desired[batch_item], \n",
    "            possible_steps_direction[batch_item].transpose(2, 1)\n",
    "        ).reshape(desired.shape[1], emb.num_embeddings)\n",
    "\n",
    "    return similarity2possible_from_desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1911e35c-860f-4012-84e2-7d255940d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод нарезает сконкатенированный вектор эмбеддингов обратно в эмбеддинги мцц-кодов, сумм транзакций и тд\n",
    "def chunk_embeddings_by_type(features, features_to_operate, embs_list, original_emb, desired):\n",
    "    desired_separated = []\n",
    "    original_separated = []\n",
    "    operated_features = []\n",
    "    operated_embeddings = []\n",
    "    \n",
    "    prev_emb_pointer = 0\n",
    "    for feature, emb in zip(features, embs_list):\n",
    "        if feature not in features_to_operate:\n",
    "            prev_emb_pointer = prev_emb_pointer + emb.embedding_dim\n",
    "            continue\n",
    "        operated_features.append(feature)\n",
    "        operated_embeddings.append(emb)\n",
    "        desired_separated.append(desired[:, :, prev_emb_pointer: prev_emb_pointer + emb.embedding_dim])\n",
    "        original_separated.append(original_emb[:, :, prev_emb_pointer: prev_emb_pointer + emb.embedding_dim])\n",
    "        prev_emb_pointer = prev_emb_pointer + emb.embedding_dim\n",
    "        \n",
    "    return desired_separated, original_separated, operated_features, operated_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc9f7bfe-a6e2-4e7f-8945-17b2807e398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# самый главный метод, оценивает, какие замены эмбеддингов в паре транзакции-суммы даст наибольший вклад в \"слом\" модели\n",
    "\n",
    "def chose_transaction_embs(original_emb, desired, embs_list, features, features_to_operate, is_pos_amt, budget=10):\n",
    "    assert len(desired.shape) == 3, desired.shape\n",
    "    assert desired.shape == original_emb.shape, (desired.shape, original_emb.shape)\n",
    "    \n",
    "    desired_separated, original_separated, operated_features, operated_embeddings = chunk_embeddings_by_type(\n",
    "        features, \n",
    "        features_to_operate, \n",
    "        embs_list, \n",
    "        original_emb, \n",
    "        desired)\n",
    "    \n",
    "    similarity2possible_from_desired = {}\n",
    "    \n",
    "    for o, d, emb, f in zip(original_separated, desired_separated, operated_embeddings, operated_features):\n",
    "        similarity2possible_from_desired[f] = _compute_best_option_for_change_emb(o, d, emb, f)\n",
    "        \n",
    "    mcc_impact_over_transactions = similarity2possible_from_desired['mcc_code']\n",
    "    amt_impact_over_transactions = similarity2possible_from_desired['transaction_amt']\n",
    "    \n",
    "    # раздуваем пространство так, чтобы появилась матрица для оценки именно вклада пар в изменение суждений модели\n",
    "    impact_scores_overall = mcc_impact_over_transactions.reshape(*mcc_impact_over_transactions.shape, 1) \\\n",
    "    + amt_impact_over_transactions.reshape(*amt_impact_over_transactions.shape[:-1], 1, amt_impact_over_transactions.shape[-1])\n",
    "    \n",
    "    assert mcc_impact_over_transactions.shape[0] == amt_impact_over_transactions.shape[0]\n",
    "    assert mcc_impact_over_transactions.shape[1] == amt_impact_over_transactions.shape[1]\n",
    "    \n",
    "    assert impact_scores_overall.shape == (\n",
    "        mcc_impact_over_transactions.shape[0], \n",
    "        mcc_impact_over_transactions.shape[1], \n",
    "        mcc_impact_over_transactions.shape[2], \n",
    "        amt_impact_over_transactions.shape[2]\n",
    "    ), impact_scores_overall.shape\n",
    "\n",
    "    best_overall_impacts = impact_scores_overall\n",
    "    \n",
    "    best_amts_ids = impact_scores_overall.argmax(dim=-1)\n",
    "    best_overall_impacts = best_overall_impacts.max(dim=-1).values\n",
    "    best_mccs_ids = best_overall_impacts.argmax(dim=-1)\n",
    "    best_overall_impacts_temp = best_overall_impacts\n",
    "    best_overall_impacts = best_overall_impacts.max(dim=-1).values\n",
    "    best_transactions = best_overall_impacts.argsort(descending=True, dim=-1)[:, :budget]\n",
    "    \n",
    "    # я не смог в тензорные колдунства с индексацией, вот уродливые методы, которые делают умную селекцию\n",
    "    def choose1(index, target):\n",
    "        result = torch.zeros(*index.shape, dtype=target.dtype, device=target.device)\n",
    "        for i in range(index.shape[0]):\n",
    "            result[i] = target[i][index[i]]\n",
    "        return result\n",
    "    \n",
    "    def choose2(index, target):\n",
    "        result = torch.zeros(*index.shape, dtype=target.dtype, device=target.device)\n",
    "        for i in range(index.shape[0]):\n",
    "            for j in range(index.shape[1]):\n",
    "                result[i, j] = target[i, j][index[i, j]]\n",
    "        return result\n",
    "    \n",
    "    def choose3(index, target):\n",
    "        result = torch.zeros(*target.shape, dtype=target.dtype, device=target.device)\n",
    "        for i in range(index.shape[0]):\n",
    "            for j in range(index.shape[1]):\n",
    "                result[i, j] = target[i, index[i, j]]\n",
    "        return result\n",
    "    \n",
    "    chosen_transactions = best_transactions\n",
    "    chosen_mccs_ids = choose1(chosen_transactions, best_mccs_ids)\n",
    "    chosen_amts_ids = choose2(chosen_mccs_ids, choose3(chosen_transactions, best_amts_ids))\n",
    "        \n",
    "    return operated_features, ((chosen_transactions), [chosen_mccs_ids, chosen_amts_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59dadd11-d8ce-4add-8ffa-7975a53cd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_best_exchange(rnn, record, features, features_to_operate, is_pos_amt, total_changes=10, to_positive=True):\n",
    "    \n",
    "    x = rnn._get_input_embs(record)\n",
    "    \n",
    "    assert len(x.shape) == 3, x.shape\n",
    "    \n",
    "    # весь матан тут\n",
    "    jac = jacobian(lambda x: rnn.classify_emb(x.shape[0], *rnn.get_emb(x)), x)\n",
    "    # больше матана не будет\n",
    "    \n",
    "    operated_features, impactor_ids = chose_transaction_embs(\n",
    "        x, \n",
    "        jac[\n",
    "            torch.arange(jac.shape[0]).to(jac.device),\n",
    "            to_positive,\n",
    "            torch.arange(jac.shape[0]).to(jac.device)], \n",
    "        rnn._transaction_cat_embeddings, \n",
    "        features, \n",
    "        features_to_operate,\n",
    "        is_pos_amt,\n",
    "        budget=total_changes\n",
    "    )\n",
    "    \n",
    "    return operated_features, impactor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "477aa406-7794-47de-b75d-9db977baf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод находит для индексов эмбеддингов исходные суммы и названия мцц кодов транзакций для каждой записи\n",
    "def uncut(original_df, original_transaction_id, transaction_relative_ids, transaction_details_ids, bins, features_to_operate):\n",
    "    transactions_to_change_ids = original_transaction_id[transaction_relative_ids]\n",
    "    for transaction_id, change_ids  in zip(transactions_to_change_ids, range(transactions_to_change_ids.shape[0])):\n",
    "        changes = dict(zip(features_to_operate, transaction_details_ids))\n",
    "\n",
    "        mcc_code_original = found_source(original2bin, 'mcc_code', changes['mcc_code'][change_ids].item())\n",
    "        amt_change_bucket = changes['transaction_amt'][change_ids].item()\n",
    "        \n",
    "        amt_change = None\n",
    "        if mcc_code_original in positive_q_df_after_digitize.index:\n",
    "            mcc_qua_record = positive_q_df_after_digitize.loc[mcc_code_original]\n",
    "            \n",
    "            if mcc_qua_record.minqua_bin <= amt_change_bucket <= mcc_qua_record.maxqua_bin:\n",
    "                bucket_amt_left_bound = bins[feature][changes['transaction_amt'][change_ids].item()]\n",
    "                bucket_amt_right_bound = bins[feature][changes['transaction_amt'][change_ids].item() + 1]\n",
    "                amt_change = (max(mcc_qua_record.minqua, bucket_amt_left_bound) + min(mcc_qua_record.maxqua, bucket_amt_right_bound))/2\n",
    "\n",
    "        if mcc_code_original in negative_q_df_after_digitize.index:\n",
    "            mcc_qua_record = negative_q_df_after_digitize.loc[mcc_code_original]\n",
    "            \n",
    "            if mcc_qua_record.minqua_bin <= amt_change_bucket <= mcc_qua_record.maxqua_bin:\n",
    "                bucket_amt_left_bound = bins[feature][changes['transaction_amt'][change_ids].item()]\n",
    "                bucket_amt_right_bound = bins[feature][changes['transaction_amt'][change_ids].item() + 1]\n",
    "                amt_change = (max(mcc_qua_record.minqua, bucket_amt_left_bound) + min(mcc_qua_record.maxqua, bucket_amt_right_bound))/2\n",
    "\n",
    "        if amt_change is None:\n",
    "            amt_change = -1  # dirty hack\n",
    "            mcc_code_original = -1\n",
    "#             raise Exception(f'mc {mcc_code_original}, amt {amt_change_bucket}')\n",
    "        \n",
    "        original_df[transaction_id.item(), features.index('transaction_amt')] = amt_change\n",
    "        original_df[transaction_id.item(), features.index('mcc_code')] = mcc_code_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e987d8d-0cfa-490c-86c3-d5d9b9da1f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# поскольку эмбеддинги мы подменяли без каких-либо ограничений, надо полечить итоговую атаку от срабатыванию бюджетных ограничений, делаем это просто заменой больных транзакций на исходные\n",
    "def heal_budget(original_transactions, harmed_transactions, quantiles):\n",
    "    assert original_transactions.shape == harmed_transactions.shape, 'nothing to do with the corrupted data'\n",
    "\n",
    "    def is_different_records(a, b):\n",
    "        return not all(\n",
    "            [\n",
    "                a.user_id == b.user_id,\n",
    "                a.mcc_code == b.mcc_code,\n",
    "                a.currency_rk == b.currency_rk,\n",
    "                np.isclose(a.transaction_amt, b.transaction_amt),\n",
    "                a.transaction_dttm == b.transaction_dttm,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    diff_count = defaultdict(int)\n",
    "    need_restore_to_original_records_ids = []\n",
    "    \n",
    "    errors_found = defaultdict(int)\n",
    "    \n",
    "    for i, (a, b) in enumerate(tqdm(zip(original_transactions.itertuples(), harmed_transactions.itertuples()), desc='looking for errors', total=original_transactions.shape[0])):\n",
    "        if is_different_records(a, b):\n",
    "            diff_count[a.user_id] += 1\n",
    "            if diff_count[a.user_id] > BUDGET:\n",
    "                errors_found['budget over'] += 1\n",
    "                need_restore_to_original_records_ids.append(i)\n",
    "                continue\n",
    "\n",
    "            if np.sign(a.transaction_amt) != np.sign(b.transaction_amt):\n",
    "                errors_found['sign change'] += 1\n",
    "                need_restore_to_original_records_ids.append(i)\n",
    "                continue\n",
    "\n",
    "            if a.transaction_amt < 0:\n",
    "                ruler = quantiles[\"negative\"]\n",
    "            else:\n",
    "                ruler = quantiles[\"positive\"]\n",
    "\n",
    "            key_b = str(b.mcc_code)\n",
    "            \n",
    "            if key_b not in ruler[\"max\"] or key_b not in ruler[\"min\"]:\n",
    "                errors_found['bad mcc'] += 1\n",
    "                need_restore_to_original_records_ids.append(i)\n",
    "                continue\n",
    "            upper_bound_b = ruler[\"max\"][key_b]\n",
    "            lower_bound_b = ruler[\"min\"][key_b]\n",
    "            if any(\n",
    "                [\n",
    "                    upper_bound_b < b.transaction_amt,\n",
    "                    lower_bound_b > b.transaction_amt,\n",
    "                ]\n",
    "            ):\n",
    "                errors_found['amt exceeded'] += 1\n",
    "                need_restore_to_original_records_ids.append(i)\n",
    "                continue\n",
    "    \n",
    "    result = harmed_transactions.copy()\n",
    "    \n",
    "    for id2restore in tqdm(need_restore_to_original_records_ids, desc='healing errors'):\n",
    "        result.iloc[id2restore] = original_transactions.iloc[id2restore]\n",
    "    \n",
    "    print(errors_found)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfbc26b3-ff20-4bd4-bc4c-2e03275dd1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3afe0853ae49dfadf2d9e891527864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load symbol cublasGetSmCountTarget from libcublas.so.11. Error: /usr/local/cuda/lib64/libcublas.so.11: undefined symbol: cublasGetSmCountTarget\n"
     ]
    }
   ],
   "source": [
    "df_transactions = (\n",
    "    pd.read_csv(\n",
    "        transactions_path,\n",
    "        parse_dates=[\"transaction_dttm\"],\n",
    "        dtype={\"user_id\": int, \"mcc_code\": int, \"currency_rk\": int, \"transaction_amt\": float}#, nrows=300*400\n",
    "    )\n",
    "    .dropna()\n",
    "    .assign(\n",
    "        hour=lambda x: x.transaction_dttm.dt.hour,\n",
    "        day=lambda x: x.transaction_dttm.dt.dayofweek,\n",
    "        month=lambda x: x.transaction_dttm.dt.month,\n",
    "        number_day=lambda x: x.transaction_dttm.dt.day,\n",
    "        is_pos_amt=lambda x: x.transaction_amt > 0\n",
    "    )\n",
    ")\n",
    "\n",
    "df = process_for_nn(df_transactions, features, bins)\n",
    "dataset = TransactionsDataset(df)\n",
    "dataloader = get_dataloader(dataset, device, batch_size=10, is_validation=True)\n",
    "\n",
    "df_transactions_np = df_transactions.values\n",
    "\n",
    "tqdm_dl = tqdm(dataloader)\n",
    "\n",
    "for xs, users, original_transaction_ids, is_pos_amt in tqdm_dl:\n",
    "    score_at_start = rnn(xs)[:, 1]\n",
    "    to_positive_mask = (score_at_start < 0.15).bool()\n",
    "\n",
    "    for _ in range(1):    \n",
    "        operated_features, (batched_transaction_relative_ids, batched_transaction_details_ids) = estimate_best_exchange(\n",
    "            rnn, \n",
    "            xs, \n",
    "            features, \n",
    "            features_to_operate,\n",
    "            is_pos_amt.bool(),\n",
    "            to_positive=to_positive_mask.long(),\n",
    "            total_changes=10\n",
    "        )\n",
    "\n",
    "        for j in range(xs.shape[0]):\n",
    "            for i, transaction2change in enumerate(batched_transaction_relative_ids[j]):\n",
    "                for feature, change_modality in zip(operated_features, [x[j] for x in batched_transaction_details_ids]):\n",
    "                    xs[j, features.index(feature), transaction2change] = change_modality[i]\n",
    "                    \n",
    "                    \n",
    "        for original_transaction_id, transaction_relative_ids, transaction_details_batch_index in zip(original_transaction_ids, batched_transaction_relative_ids, range(original_transaction_ids.shape[0])):\n",
    "            uncut(\n",
    "                df_transactions_np, \n",
    "                original_transaction_id.cpu(), \n",
    "                transaction_relative_ids.cpu(), \n",
    "                [x[transaction_details_batch_index].cpu() for x in batched_transaction_details_ids], \n",
    "                bins, \n",
    "                operated_features)\n",
    "\n",
    "for i, column in enumerate(df_transactions.columns):\n",
    "    df_transactions[column] = df_transactions_np[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "820fb36a-6d1a-4a30-9e9c-3aa58bb577c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmed_transactions = df_transactions.drop(['hour', 'day', 'month', 'number_day', 'is_pos_amt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcafc03c-cf12-46f9-a34c-103b9c0ebfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_transactions = pd.read_csv(\n",
    "    transactions_path,\n",
    "    parse_dates=[\"transaction_dttm\"],\n",
    "    dtype={\"user_id\": int, \"mcc_code\": int, \"currency_rk\": int, \"transaction_amt\": float}#, nrows=300*400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7013495c-fc2a-489d-bbfb-85a4a38e7abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5618c0038f524085942f8ae0ae0dbb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "looking for errors:   0%|          | 0/1260000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462b8712be22483e9167fa1f4816df80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "healing errors:   0%|          | 0/1082 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'sign change': 1082})\n"
     ]
    }
   ],
   "source": [
    "healed_dangerous_transactions = heal_budget(original_transactions, harmed_transactions, quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f4a7168-bd2a-4b1a-99ec-58f6cf18a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "healed_dangerous_transactions.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11e565c5-951a-449e-86ac-f6d2b324102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1260000/1260000 [00:44<00:00, 28504.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_budget(transactions_path, output_path, quantiles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173a18c-193a-4fdf-92b6-c1315315b1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d8c3d-25f5-491e-b06e-789fb5d08fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd632a-72d9-489e-b225-0981b4c7d537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00052c6a-e7ab-4786-89bc-78ee9af9f2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb1aed-f92e-4ef1-b2e7-fb86f46a99ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52906721-0d38-4e14-aad1-45e63cd604d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d6e34-7082-46f3-868f-3f9c930a6823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03376d-aeea-481c-9991-83f6f0f3e37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
